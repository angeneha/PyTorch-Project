---
title: "PyTorch Progress"
author: "Angene"
date: "April 10, 2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

#Goal: Build a pytorch model to predict dna sequence score, to predict a label from a sequence that we program
#Reference: DNA Pytorch by Erin wilson


#Adding the libraries and modules for pytorch use
```
from collections import defaultdict
from intertools import product
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import torch
from torch import nn
```

 
#Set a random seed, an integer parameter, across the libraries and modules within this program to ensure random reproducibility of the results.
```
def set_seed(seed: int = 42) -> None:
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"Random seed set as {seed}")
    
set_seed(17)
```
#The random seed is now set at 17


#This block will select the appropriate device, GPU or CPU based on the system that pytorch was imported in 
```
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
DEVICE
```

##DESIREE Define the DNA sequence dataset by generating a score for each nucleotide. This section of code block is used to assign sequences a score for pytorch. For every 8-mer sequence the nucleotide sum based on the rule system, the assignment of a number to a letter, will be added and averaged to produce the score of the sequence.

```
def kmers(k):
    '''Generate a list of all k-mers for a given k'''

    return [''.join(x) for x in product(['A','C','G','T'], repeat=k)]
```


#DESIREE prints the Total of 8mers
```
seqs8 = kmers(8)
print('Total 8mers:',len(seqs8))
```

#This code block creates a way to assign scores to DNA sequences based on the individual scores of nucleotides and additional adjustments for specific motifs within the sequences
```
score_dict = {
    'A':20,
    'C':17,
    'G':14,
    'T':11
}
                  
def score_seqs_motif(seqs):
    '''
    Calculate the scores for a list of sequences based on 
    the above score_dict
    '''
    data = []
    for seq in seqs:
        #DESIREE get the average score by nucleotide
        score = np.mean([score_dict[base] for base in seq])
        
        #DESIREE give a + or - bump if this k-mer has a specific motif
        if 'TAT' in seq:
            score += 10
        if 'GCG' in seq:
            score -= 10
        data.append([seq,score])
        
    df = pd.DataFrame(data, columns=['seq','score'])
    return df
```


```
mer8 = score_seqs_motif(seqs8)
```


```
def one_hot_encode(seq):
    """
    Given a DNA sequence, return its one-hot encoding
    """
    #DESIREE Make sure the sequence has only allowed bases
    allowed = set("ACTGN")
    if not set(seq).issubset(allowed):
        invalid = set(seq) - allowed
        raise ValueError(f"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}")

#DESIREE Dictionary returning one-hot encoding for each nucleotide 
    nuc_d = {'A':[1.0,0.0,0.0,0.0],
             'C':[0.0,1.0,0.0,0.0],
             'G':[0.0,0.0,1.0,0.0],
             'T':[0.0,0.0,0.0,1.0],
             'N':[0.0,0.0,0.0,0.0]}

  #DESIREE Create array from nucleotide sequence
    vec=np.array([nuc_d[x] for x in seq])
        
    return vec
```


#DESIREE look at DNA seq of 8 As
a8 = one_hot_encode("AAAAAAAA")
print("AAAAAAAA:\n",a8)

#prints:
#AAAAAAAA:
#[[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]
#[1. 0. 0. 0.]]

#look at DNA seq of random nucleotides
s = one_hot_encode("AGGTACCT")
print("AGGTACCT:\n",s)
print("shape:",s.shape)

#prints:
#AGGTACCT:
#[[1. 0. 0. 0.]
#[0. 0. 1. 0.]
#[0. 0. 1. 0.]
#[0. 0. 0. 1.]
#[1. 0. 0. 0.]
#[0. 1. 0. 0.]
#[0. 1. 0. 0.]
#[0. 0. 0. 1.]]
#shape: (8, 4)

def quick_split(df, split_frac=0.8):
    '''
    Given a df of samples, randomly split indices between
    train and test at the desired fraction
    '''
    cols = df.columns # original columns, use to clean up reindexed cols
    df = df.reset_index()

    #shuffle indices
    idxs = list(range(df.shape[0]))
    random.shuffle(idxs)

    #split shuffled index list by split_frac
    split = int(len(idxs)*split_frac)
    train_idxs = idxs[:split]
    test_idxs = idxs[split:]
    
    #split dfs and return
    train_df = df[df.index.isin(train_idxs)]
    test_df = df[df.index.isin(test_idxs)]
        
    return train_df[cols], test_df[cols]
    
    
full_train_df, test_df = quick_split(mer8)
train_df, val_df = quick_split(full_train_df)

print("Train:", train_df.shape)
print("Val:", val_df.shape)
print("Test:", test_df.shape)

#prints: 
#Train: (41942, 2)
#Val: (10486, 2)
#Test: (13108, 2)

#ALEJANDRA starting to define Pytorch models #
#reference: "Modeling DNA Sequences with PyTorch" by Erin Wilson #
#simple linear model #
class DNA_Linear(nn.Module):
    def __init__(self, seq_len):
        super().__init__()
        self.seq_len = seq_len
        # the 4 id for our one-hot encoded vector length 4! #
        self.lin = nn.Linear(4*seq_len, 1)

    def forward(self, xb):
        # reshape to flatten sequence dimension #
        xb = xb.view(xb.shape[0],self.seq_len*4)
        # Linear wraps up the weights/bias dot product operations #
        out = self.lin(xb)
        return out

# basic CNN model #
class DNA_CNN(nn.Module):
    def __init__(self,
                 seq_len,
                 num_filters=32,
                 kernel_size=3):
       super().__init__()
       self.seq_len = seq_len

       self.conv_net = nn.Sequential(
           # 4 is for the 4 nucleotides #
           nn.Conv1d(4, num_filters, kernel_size=kernel_size),
           nn.ReLu(inplace=True),
           nn.Flatten(),
           nn.Linear(num_filters*(seq_len-kernel_size+1), 1)
       )

    def forward(self, xb):
        # permute to put channel in correct order #
        # (batch_size x 4channel x seq_len) #
        xb = xb.permute(0,2,1)

        # print(xb.shape) #
        out = self.conv_net(xb)
        return out

# start of training loops #
# | Training and fitting functions | #

def loss_batch(model, loss_func, xb, yb, opt=None, verbose=False):
    '''
    Apply loss function to a batch of inputs. If no optimizer
    is provided, skip the back propogation step.
    '''
    if verbose:
        print('loss batch ****')
        print("xb shape:",xb.shape)
        print("yb shape:",yb.shape)
        print("yb shape:",yb.squeeze(1).shape)
        #print("yb",yb)

    # get the batch output from the model given your input batch #
    # ** This is the model's prediction for the y labels #
    xb_out = model(xb.float())

    if verbose:
        print("model out pre loss", xb_out.shape)
        #print('xb_out', xb_out) #
        print("xb_out:",xb_out.shape)
        print("yb:",yb.shape)
        print("yb.long:",yb.long().shape)

    loss = loss_func(xb_out, yb.float()) # for MSE/regression #
    # __FOOTNOTE 2__ #

    if opt is not None: # if opt #
        loss.backward()
        opt.step()
        opt.zero_grad()

    return loss.item(), len(xb)

def train_step(model, train_dl, loss_func, device, opt):
    '''
    Execute 1 set of batched training within an epoch 
    '''
    # Set model to training mode #
    model.train()
    tl = [] # train losses
    ns = [] # batch sizes, n

    # loop through train DataLoader
    for xb, yb in train_dl:
        # put on GPU
        xb, yb = xb.to(device),yb.to(device)

        # provide opt so backprop happens
        t,n = loss_batch(model, loss_func, xb, yb, opt=opt)

        # collect train loss and batch sizes
        tl.append(t)
        ns.append(n)

    # average the losses over all batches
    train_loss = np.sum(np.multiply(tl,ns)) / np.sum(ns)

    return train_loss

def val_step(model, val_dl, loss_func, device):
    '''
    Excecute 1 set of batched validation within an epoch
    '''
    # Set model to Evaluation mode
    model.eval()
    with torch.no_grad():
        vl = [] # val losses
        ns = [] # batch sizes, n

        # loop through validation DataLoader
        for xb, yb in val_dl:
            # put on GPU
            xb, yb = xb.to(device),yb.to(device)

            # Do NOT provide opt here, so backprop doesn't happen
            v, n = loss_batch(model, loss_func, xb, yb)

            # collect val loss and batch sizes
            vl.append(v)
            ns.append(n)

    # average the losses over all batches
    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)

    return val_loss

def fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):
    '''
    Fit the model params to the training data, eval on unseen data.
    Loop for a number of epochs and keep train?typo? of train and val
    losses along the way
    '''
    # keep track of losses
    train_losses = []
    val_losses = []

    # loop through epochs
    for epoch in range(epochs):
        # take a training step
        train_loss = train_step(model,train_dl,loss_func,device,opt)
        train_losses.append(train_loss)

        # take a validation step
        val_loss = val_step(model,val_dl,loss_func,device)
        val_losses.append(val_loss)

        print(f"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}")

    return train_losses, val_losses


def run_model(train_dl,val_dl,model,device,
              lr=0.01, epochs=50,
              lossf=None,opt=None
             ):
    '''
    Given train and val DataLoaders and a NN model, fit the mode to the
    training data. By default, use MSE loss and an SGD optimizer
    '''
    # define optimizer
    if opt:
        optimizer = opt
    else: # if no opt provided, just use SGD
        optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    # define loss function
    if lossf:
        loss_func = lossf
    else: # if no loss function provided, just use MSE
        loss_func = torch.nn.MSELoss()

    # run the training loop
    train_losses, val_losses = fit(
                                epochs,
                                model,
                                loss_func,
                                optimizer,
                                train_dl,
                                val_dl,
                                device)

   return train_losses, val_losses

# use GPU if available
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# get the sequence length from the first seq in the df
seq_len = len(train_df['seq'].values[0])

# create Linear model object
model_lin = DNA_Linear(seq_len)
model_lin.to(DEVICE) # put on GPU

# run the model with default settings!
lin_train_losses, lin_val_losses = run_model(
    train_dl,
    val_dl,
    model_lin,
    DEVICE
)



# ANGENE starting convolutional filters
import logomaker

def get_conv_layers_from_model(model):
    '''
    Given a trained model, extract its convolutional layers
    '''
    model_children = list(model.children())
    
# counter to keep count of the conv layers
    model_weights = [] # we will save the conv layer weights in this list
    conv_layers = [] # we will save the actual conv layers in this list
    bias_weights = []
    counter = 0 

# all the conv layers and their respective weights to the list
    for i in range(len(model_children)):
# get model type of Conv1d
     if type(model_children[i]) == nn.Conv1d:
    counter += 1
    model_weights.append(model_children[i].weight)
    conv_layers.append(model_children[i])
    bias_weights.append(model_children[i].bias)

    # also checking sequential objects' children for conv1d
        elif type(model_children[i]) == nn.Sequential:
        for child in model_children[i]:
        if type(child) == nn.Conv1d:
        counter += 1
        model_weights.append(child.weight)
        conv_layers.append(child)
        bias_weights.append(child.bias)

    print(f"Total convolutional layers: {counter}")
    return conv_layers, model_weights, bias_weights

def view_filters(model_weights, num_cols=8):
    model_weights = model_weights[0]
    num_filt = model_weights.shape[0]
    filt_width = model_weights[0].shape[1]
    num_rows = int(np.ceil(num_filt/num_cols))
    
 # visualizing the first conv. layer filters
    plt.figure(figsize=(20, 17))

    for i, filter in enumerate(model_weights):
        ax = plt.subplot(num_rows, num_cols, i+1)
        ax.imshow(filter.cpu().detach(), cmap='gray')
        ax.set_yticks(np.arange(4))
        ax.set_yticklabels(['A', 'C', 'G','T'])
        ax.set_xticks(np.arange(filt_width))
        ax.set_title(f"Filter {i}")

    plt.tight_layout()
    plt.show()
    
conv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)
view_filters(model_weights)

def get_conv_output_for_seq(seq, conv_layer):
    '''
    Given an input sequeunce and a convolutional layer, 
    get the output tensor containing the conv filter 
    activations along each position in the sequence
    '''
    # format seq for input to conv layer (OHE, reshape)
    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)

    # run seq through conv layer
    with torch.no_grad(): # don't want as part of gradient graph
        # apply learned filters to input seq
        res = conv_layer(seq.float())
        return res[0]
    

def get_filter_activations(seqs, conv_layer,act_thresh=0):
    '''
    Given a set of input sequences and a trained convolutional layer, 
    determine the subsequences for which each filter in the conv layer 
    activate most strongly. 
    
    1.) Run seq inputs through conv layer. 
    2.) Loop through filter activations of the resulting tensor, saving the
            position where filter activations were > act_thresh. 
    3.) Compile a count matrix for each filter by accumulating subsequences which
            activate the filter above the threshold act_thresh
    '''
    # initialize dict of pwms for each filter in the conv layer
    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s
    num_filters = conv_layer.out_channels
    filt_width = conv_layer.kernel_size[0]
    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))
    
    print("Num filters", num_filters)
    print("filt_width", filt_width)
    
    # loop through a set of sequences and collect subseqs where each filter activated
    for seq in seqs:
        # get a tensor of each conv filter activation along the input seq
        res = get_conv_output_for_seq(seq, conv_layer)

        # for each filter and it's activation vector
        for filt_id,act_vec in enumerate(res):
            # collect the indices where the activation level 
            # was above the threshold
            act_idxs = torch.where(act_vec>act_thresh)[0]
            activated_positions = [x.item() for x in act_idxs]

            # use activated indicies to extract the actual DNA
            # subsequences that caused filter to activate
            for pos in activated_positions:
                subseq = seq[pos:pos+filt_width]
                #print("subseq",pos, subseq)
                # transpose OHE to match PWM orientation
                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T

                # add this subseq to the pwm count for this filter
                filter_pwms[filt_id] += subseq_tensor            
            
    return filter_pwms

def view_filters_and_logos(model_weights,filter_activations, num_cols=8):
    '''
    Given some convolutional model weights and filter activation PWMs, 
    visualize the heatmap and motif logo pairs in a simple grid
    '''
    model_weights = model_weights[0].squeeze(1)
    print(model_weights.shape)

    # make sure the model weights agree with the number of filters
    assert(model_weights.shape[0] == len(filter_activations))
    
    num_filts = len(filter_activations)
    num_rows = int(np.ceil(num_filts/num_cols))*2+1 
    # ^ not sure why +1 is needed... complained otherwise
    
    plt.figure(figsize=(20, 17))

    j=0 # use to make sure a filter and it's logo end up vertically paired
    for i, filter in enumerate(model_weights):
        if (i)%num_cols == 0:
            j += num_cols

        # display raw filter
        ax1 = plt.subplot(num_rows, num_cols, i+j+1)
        ax1.imshow(filter.cpu().detach(), cmap='gray')
        ax1.set_yticks(np.arange(4))
        ax1.set_yticklabels(['A', 'C', 'G','T'])
        ax1.set_xticks(np.arange(model_weights.shape[2]))
        ax1.set_title(f"Filter {i}")

        # display sequence logo
        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)
        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])
        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')
        logo = logomaker.Logo(filt_df_info,ax=ax2)
        ax2.set_ylim(0,2)
        ax2.set_title(f"Filter {i}")

    plt.tight_layout()
    

# using some seqs from test_df to activate filters
some_seqs = random.choices(seqs, k=3000)

filter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)
view_filters_and_logos(model_weights,filter_activations)


