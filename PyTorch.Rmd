---
title: "PyTorch Progress"
author: "Angene"
date: "April 10, 2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

#Goal: Build a pytorch model to predict dna sequence score, to predict a label from a sequence that we program
#Reference: DNA Pytorch by Erin wilson


#Adding the libraries and modules for pytorch use
```
from collections import defaultdict
from intertools import product
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import random
import torch
from torch import nn
```

 
#Set a random seed, an integer parameter, across the libraries and modules within this program to ensure random reproducibility of the results.
```
def set_seed(seed: int = 42) -> None:
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

    print(f"Random seed set as {seed}")
    
set_seed(17)
```
#The random seed is now set at 17


#This block will select the appropriate device, GPU or CPU based on the system that pytorch was imported in, my device is CPU
```
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
DEVICE
```

##DESIREE Define the DNA sequence dataset by generating a score for each nucleotide. This section of code block is used to assign sequences a score for pytorch. For every 8-mer sequence the nucleotide sum based on the rule system, the assignment of a number to a letter, will be added and averaged to produce the score of the sequence.

```
from itertools import product

def kmers(k):
    '''Generate all k-mers for a given k'''
    return [''.join(x) for x in product('ACGT', repeat=k)]

```


#DESIREE prints the Total of 8mers
```
seqs8 = kmers(8)
print('Total 8mers:', len(seqs8))
```

#This code block creates a way to assign scores to DNA sequences based on the individual scores of nucleotides and additional adjustments for specific motifs within the sequences
```
import numpy as np
import pandas as pd

score_dict = {
    'A': 20,
    'C': 17,
    'G': 14,
    'T': 11
}

def score_seqs_motif(seqs):
    '''
    Calculate the scores for a list of sequences based on 
    the above score_dict
    '''
    data = []
    for seq in seqs:
        # get the average score by nucleotide
        score = np.mean([score_dict[base] for base in seq])
        
        # give a + or - bump if this k-mer has a specific motif
        score += 10 if 'TAT' in seq else 0
        score -= 10 if 'GCG' in seq else 0
        
        data.append([seq, score])
        
    df = pd.DataFrame(data, columns=['seq', 'score'])
    return df
```

#Creates a chart of sequence scores for the first 4 8-mers that the program creates
```
mer8 = score_seqs_motif(seqs8)
print(mer8.head())
```

#Prints scores for the 8-mer sequences that are given to it. Used to check program
```
filtered_mer8 = mer8[mer8['seq'].isin(['TGCGTTTT', 'CCCCCTAT'])]
print(filtered_mer8)
```

#Creates a histogram plot using Matplotlib showing the distribution of scores among the 8-mer sequences.20 bins allows for a reasonably detailed visualization of the score distribution among the 8-mer sequences with specific motifs


```
import matplotlib.pyplot as plt

# Plot the histogram with seagreen bars
plt.hist(mer8['score'].values, bins=20, color='seagreen')

# Add title and labels
plt.title("Analysis of Score Distribution for 8-mer Motifs")
plt.xlabel("Score Distribution of 8-mers", fontsize=14)
plt.ylabel("Occurance of 8-mer's", fontsize=14)

# Shows the plot
plt.show()
```

```
def one_hot_encode(seq):
    """
    Given a DNA sequence, return its one-hot encoding
    """
    #DESIREE Make sure the sequence has only allowed bases
    allowed = set("ACTGN")
    if not set(seq).issubset(allowed):
        invalid = set(seq) - allowed
        raise ValueError(f"Sequence contains chars not in allowed DNA alphabet (ACGTN): {invalid}")

#DESIREE Dictionary returning one-hot encoding for each nucleotide 
    nuc_d = {'A':[1.0,0.0,0.0,0.0],
             'C':[0.0,1.0,0.0,0.0],
             'G':[0.0,0.0,1.0,0.0],
             'T':[0.0,0.0,0.0,1.0],
             'N':[0.0,0.0,0.0,0.0]}

  #DESIREE Create array from nucleotide sequence
    vec=np.array([nuc_d[x] for x in seq])
        
    return vec
```


#look at DNA seq of 8 As
```
a8 = one_hot_encode("AAAAAAAA")
print("AAAAAAAA:\n",a8)

#look at DNA seq of random nucleotides
s = one_hot_encode("AGGTACCT")
print("AGGTACCT:\n",s)
print("shape:",s.shape)
```

```
def quick_split(df, split_frac=0.8, verbose=False):
    '''
    Given a df of samples, randomly split indices between
    train and test at the desired fraction
    '''
    cols = df.columns # original columns, use to clean up reindexed cols
    df = df.reset_index()

    # shuffle indices
    idxs = list(range(df.shape[0]))
    random.shuffle(idxs)

    # split shuffled index list by split_frac
    split = int(len(idxs)*split_frac)
    train_idxs = idxs[:split]
    test_idxs = idxs[split:]
    
    # split dfs and return
    train_df = df[df.index.isin(train_idxs)]
    test_df = df[df.index.isin(test_idxs)]
        
    return train_df[cols], test_df[cols]
```

```    
def split_and_print(df):
    train_df, val_df = quick_split(df)
    print("Train:", train_df.shape)
    print("Val:", val_df.shape)
    return train_df, val_df

full_train_df, test_df = quick_split(mer8)
train_df, val_df = split_and_print(full_train_df)

print("Test:", test_df.shape)
train_df.head()
```

#Creates a purple, blue, and pink bargraph
```
import matplotlib.pyplot as plt

def plot_train_test_hist(train_df, val_df, test_df, bins=20):
    ''' Check distribution of train/test scores, sanity check that its not skewed'''
    plt.hist(train_df['score'].values, bins=bins, label='Training Set', alpha=0.5, color='blue')
    plt.hist(val_df['score'].values, bins=bins, label='Validation Set', alpha=0.75, color='purple')
    plt.hist(test_df['score'].values, bins=bins, label='Test Set', alpha=0.4, color='pink')
    plt.legend()
    plt.xlabel("Sequence Score Value", fontsize=14)
    plt.ylabel("Number of Sequences", fontsize=14)
    plt.show()
```


#Creates a plot visualizing the distribution of sequences with the given sequence scores
```
plot_train_test_hist(train_df, val_df,test_df)
``

```
import statsmodels.formula.api as smf
```

```
from torch.utils.data import Dataset, DataLoader
```

```
class SeqDatasetOHE(Dataset):
    '''
    Dataset for one-hot-encoded sequences
    '''
    def __init__(self,
                 df,
                 seq_col='seq',
                 target_col='score'
                ):
        # +--------------------+
        # | Get the X examples |
        # +--------------------+
        # extract the DNA from the appropriate column in the df
        self.seqs = list(df[seq_col].values)
        self.seq_len = len(self.seqs[0])
        
        # one-hot encode sequences, then stack in a torch tensor
        self.ohe_seqs = torch.stack([torch.tensor(one_hot_encode(x)) for x in self.seqs])
    
        # +------------------+
        # | Get the Y labels |
        # +------------------+
        self.labels = torch.tensor(list(df[target_col].values)).unsqueeze(1)
        
    def __len__(self): return len(self.seqs)
    
    def __getitem__(self,idx):
        # Given an index, return a tuple of an X with it's associated Y
        # This is called inside DataLoader
        seq = self.ohe_seqs[idx]
        label = self.labels[idx]
        
        return seq, label
```

```
def build_dataloaders(train_df,
                      test_df,
                      seq_col='seq',
                      target_col='score',
                      batch_size=128,
                      shuffle=True
                     ):
    '''
    Given a train and test df with some batch construction
    details, put them into custom SeqDatasetOHE() objects. 
    Give the Datasets to the DataLoaders and return.
    '''
    
    # create Datasets    
    train_ds = SeqDatasetOHE(train_df,seq_col=seq_col,target_col=target_col)
    test_ds = SeqDatasetOHE(test_df,seq_col=seq_col,target_col=target_col)

    # Put DataSets into DataLoaders
    train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=shuffle)
    test_dl = DataLoader(test_ds, batch_size=batch_size)

    
    return train_dl,test_dl
```

```
train_dl, val_dl = build_dataloaders(train_df, val_df)
```

#ALEJANDRA starting to define Pytorch models #
#reference: "Modeling DNA Sequences with PyTorch" by Erin Wilson #
#simple linear model #
class DNA_Linear(nn.Module):
    def __init__(self, seq_len):
        super().__init__()
        self.seq_len = seq_len
        # the 4 id for our one-hot encoded vector length 4! #
        self.lin = nn.Linear(4*seq_len, 1)

    def forward(self, xb):
        # reshape to flatten sequence dimension #
        xb = xb.view(xb.shape[0],self.seq_len*4)
        # Linear wraps up the weights/bias dot product operations #
        out = self.lin(xb)
        return out

# basic CNN model #
class DNA_CNN(nn.Module):
    def __init__(self,
                 seq_len,
                 num_filters=32,
                 kernel_size=3):
       super().__init__()
       self.seq_len = seq_len

       self.conv_net = nn.Sequential(
           # 4 is for the 4 nucleotides #
           nn.Conv1d(4, num_filters, kernel_size=kernel_size),
           nn.ReLu(inplace=True),
           nn.Flatten(),
           nn.Linear(num_filters*(seq_len-kernel_size+1), 1)
       )

    def forward(self, xb):
        # permute to put channel in correct order #
        # (batch_size x 4channel x seq_len) #
        xb = xb.permute(0,2,1)

        # print(xb.shape) #
        out = self.conv_net(xb)
        return out

# start of training loops #
# | Training and fitting functions | #

def loss_batch(model, loss_func, xb, yb, opt=None, verbose=False):
    '''
    Apply loss function to a batch of inputs. If no optimizer
    is provided, skip the back propogation step.
    '''
    if verbose:
        print('loss batch ****')
        print("xb shape:",xb.shape)
        print("yb shape:",yb.shape)
        print("yb shape:",yb.squeeze(1).shape)
        #print("yb",yb)

    # get the batch output from the model given your input batch #
    # ** This is the model's prediction for the y labels #
    xb_out = model(xb.float())

    if verbose:
        print("model out pre loss", xb_out.shape)
        #print('xb_out', xb_out) #
        print("xb_out:",xb_out.shape)
        print("yb:",yb.shape)
        print("yb.long:",yb.long().shape)

    loss = loss_func(xb_out, yb.float()) # for MSE/regression #
    # __FOOTNOTE 2__ #

    if opt is not None: # if opt #
        loss.backward()
        opt.step()
        opt.zero_grad()

    return loss.item(), len(xb)

def train_step(model, train_dl, loss_func, device, opt):
    '''
    Execute 1 set of batched training within an epoch 
    '''
    # Set model to training mode #
    model.train()
    tl = [] # train losses
    ns = [] # batch sizes, n

    # loop through train DataLoader
    for xb, yb in train_dl:
        # put on GPU
        xb, yb = xb.to(device),yb.to(device)

        # provide opt so backprop happens
        t,n = loss_batch(model, loss_func, xb, yb, opt=opt)

        # collect train loss and batch sizes
        tl.append(t)
        ns.append(n)

    # average the losses over all batches
    train_loss = np.sum(np.multiply(tl,ns)) / np.sum(ns)

    return train_loss

def val_step(model, val_dl, loss_func, device):
    '''
    Excecute 1 set of batched validation within an epoch
    '''
    # Set model to Evaluation mode
    model.eval()
    with torch.no_grad():
        vl = [] # val losses
        ns = [] # batch sizes, n

        # loop through validation DataLoader
        for xb, yb in val_dl:
            # put on GPU
            xb, yb = xb.to(device),yb.to(device)

            # Do NOT provide opt here, so backprop doesn't happen
            v, n = loss_batch(model, loss_func, xb, yb)

            # collect val loss and batch sizes
            vl.append(v)
            ns.append(n)

    # average the losses over all batches
    val_loss = np.sum(np.multiply(vl, ns)) / np.sum(ns)

    return val_loss

def fit(epochs, model, loss_func, opt, train_dl, val_dl,device,patience=1000):
    '''
    Fit the model params to the training data, eval on unseen data.
    Loop for a number of epochs and keep train?typo? of train and val
    losses along the way
    '''
    # keep track of losses
    train_losses = []
    val_losses = []

    # loop through epochs
    for epoch in range(epochs):
        # take a training step
        train_loss = train_step(model,train_dl,loss_func,device,opt)
        train_losses.append(train_loss)

        # take a validation step
        val_loss = val_step(model,val_dl,loss_func,device)
        val_losses.append(val_loss)

        print(f"E{epoch} | train loss: {train_loss:.3f} | val loss: {val_loss:.3f}")

    return train_losses, val_losses


def run_model(train_dl,val_dl,model,device,
              lr=0.01, epochs=50,
              lossf=None,opt=None
             ):
    '''
    Given train and val DataLoaders and a NN model, fit the mode to the
    training data. By default, use MSE loss and an SGD optimizer
    '''
    # define optimizer
    if opt:
        optimizer = opt
    else: # if no opt provided, just use SGD
        optimizer = torch.optim.SGD(model.parameters(), lr=lr)

    # define loss function
    if lossf:
        loss_func = lossf
    else: # if no loss function provided, just use MSE
        loss_func = torch.nn.MSELoss()

    # run the training loop
    train_losses, val_losses = fit(
                                epochs,
                                model,
                                loss_func,
                                optimizer,
                                train_dl,
                                val_dl,
                                device)

   return train_losses, val_losses

# use GPU if available
DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# get the sequence length from the first seq in the df
seq_len = len(train_df['seq'].values[0])

# create Linear model object
model_lin = DNA_Linear(seq_len)
model_lin.to(DEVICE) # put on GPU

# run the model with default settings!
lin_train_losses, lin_val_losses = run_model(
    train_dl,
    val_dl,
    model_lin,
    DEVICE
)



# ANGENE is evaluating the model predictions on the test set and using parity plots to visualize the difference between the actual test sequence scores vs the modelâ€™s predicted scores.
import altair as alt
from sklearn.metrics import r2_score

def parity_plot(model_name,df,r2):
    '''
    Given a dataframe of samples with their true and predicted values,
    make a scatterplot.
    '''
    plt.scatter(df['truth'].values, df['pred'].values, alpha=0.2)
    
    # y=x line
    xpoints = ypoints = plt.xlim()
    plt.plot(xpoints, ypoints, linestyle='--', color='k', lw=2, scalex=False, scaley=False)

    plt.ylim(xpoints)
    plt.ylabel("Predicted Score",fontsize=14)
    plt.xlabel("Actual Score",fontsize=14)
    plt.title(f"{model_name} (r2:{r2:.3f})",fontsize=20)
    plt.show()
    
def alt_parity_plot(model,df, r2):
    '''
    Make an interactive parity plot with altair
    '''
    chart = alt.Chart(df).mark_circle(size=100,opacity=0.4).encode(
        alt.X('truth:Q'),
        alt.Y('pred:Q'),
        tooltip=['seq:N']
    ).properties(
        title=f'{model} (r2:{r2:.3f})'
    ).interactive()
    
    chart.save(f'alt_out/parity_plot_{model}.html')
    display(chart)

def parity_pred(models, seqs, oracle,alt=False):
    '''Given some sequences, get the model's predictions '''
    dfs = {} # key: model name, value: parity_df
    
    for model_name,model in models:
        print(f"Running {model_name}")
        data = []
        for dna in seqs:
            s = torch.tensor(one_hot_encode(dna)).unsqueeze(0).to(DEVICE)
            actual = oracle[dna]
            pred = model(s.float())
            data.append([dna,actual,pred.item()])
        df = pd.DataFrame(data, columns=['seq','truth','pred'])
        r2 = r2_score(df['truth'],df['pred'])
        dfs[model_name] = (r2,df)
        
        #plot parity plot
        if alt: # make an altair plot
            alt_parity_plot(model_name, df, r2)
        else:
            parity_plot(model_name, df, r2)
            
# generate plots
seqs = test_df['seq'].values
models = [
    ("Linear", model_lin),
    ("CNN", model_cnn)
]
parity_pred(models, seqs, oracle)
#Linear model predicts a trend in the Test set sequences.The CNN is better at predicting scores close to the actual value.

#starting visualization of convolutional filters to understand more about what the model is learning
import logomaker

def get_conv_layers_from_model(model):
    '''
    Given a trained model, extract its convolutional layers
    '''
    model_children = list(model.children())
    
# counter to keep count of the conv layers
    model_weights = [] # we will save the conv layer weights in this list
    conv_layers = [] # we will save the actual conv layers in this list
    bias_weights = []
    counter = 0 

# all the conv layers and their respective weights to the list
    for i in range(len(model_children)):
# get model type of Conv1d
     if type(model_children[i]) == nn.Conv1d:
    counter += 1
    model_weights.append(model_children[i].weight)
    conv_layers.append(model_children[i])
    bias_weights.append(model_children[i].bias)

    # also checking sequential objects' children for conv1d
        elif type(model_children[i]) == nn.Sequential:
        for child in model_children[i]:
        if type(child) == nn.Conv1d:
        counter += 1
        model_weights.append(child.weight)
        conv_layers.append(child)
        bias_weights.append(child.bias)

    print(f"Total convolutional layers: {counter}")
    return conv_layers, model_weights, bias_weights

def view_filters(model_weights, num_cols=8):
    model_weights = model_weights[0]
    num_filt = model_weights.shape[0]
    filt_width = model_weights[0].shape[1]
    num_rows = int(np.ceil(num_filt/num_cols))
    
 # visualizing the first conv. layer filters
    plt.figure(figsize=(20, 17))

    for i, filter in enumerate(model_weights):
        ax = plt.subplot(num_rows, num_cols, i+1)
        ax.imshow(filter.cpu().detach(), cmap='gray')
        ax.set_yticks(np.arange(4))
        ax.set_yticklabels(['A', 'C', 'G','T'])
        ax.set_xticks(np.arange(filt_width))
        ax.set_title(f"Filter {i}")

    plt.tight_layout()
    plt.show()
    
conv_layers, model_weights, bias_weights = get_conv_layers_from_model(model_cnn)
view_filters(model_weights)

def get_conv_output_for_seq(seq, conv_layer):
    '''
    Given an input sequeunce and a convolutional layer, 
    get the output tensor containing the conv filter 
    activations along each position in the sequence
    '''
    # format seq for input to conv layer (OHE, reshape)
    seq = torch.tensor(one_hot_encode(seq)).unsqueeze(0).permute(0,2,1).to(DEVICE)

    # run seq through conv layer
    with torch.no_grad(): # don't want as part of gradient graph
        # apply learned filters to input seq
        res = conv_layer(seq.float())
        return res[0]
    

def get_filter_activations(seqs, conv_layer,act_thresh=0):
    '''
    Given a set of input sequences and a trained convolutional layer, 
    determine the subsequences for which each filter in the conv layer 
    activate most strongly. 
    
    1.) Run seq inputs through conv layer. 
    2.) Loop through filter activations of the resulting tensor, saving the
            position where filter activations were > act_thresh. 
    3.) Compile a count matrix for each filter by accumulating subsequences which
            activate the filter above the threshold act_thresh
    '''
    # initialize dict of pwms for each filter in the conv layer
    # pwm shape: 4 nucleotides X filter width, initialize to 0.0s
    num_filters = conv_layer.out_channels
    filt_width = conv_layer.kernel_size[0]
    filter_pwms = dict((i,torch.zeros(4,filt_width)) for i in range(num_filters))
    
    print("Num filters", num_filters)
    print("filt_width", filt_width)
    
    # loop through a set of sequences and collect subseqs where each filter activated
    for seq in seqs:
        # get a tensor of each conv filter activation along the input seq
        res = get_conv_output_for_seq(seq, conv_layer)

        # for each filter and it's activation vector
        for filt_id,act_vec in enumerate(res):
            # collect the indices where the activation level 
            # was above the threshold
            act_idxs = torch.where(act_vec>act_thresh)[0]
            activated_positions = [x.item() for x in act_idxs]

            # use activated indicies to extract the actual DNA
            # subsequences that caused filter to activate
            for pos in activated_positions:
                subseq = seq[pos:pos+filt_width]
                #print("subseq",pos, subseq)
                # transpose OHE to match PWM orientation
                subseq_tensor = torch.tensor(one_hot_encode(subseq)).T

                # add this subseq to the pwm count for this filter
                filter_pwms[filt_id] += subseq_tensor            
            
    return filter_pwms

def view_filters_and_logos(model_weights,filter_activations, num_cols=8):
    '''
    Given some convolutional model weights and filter activation PWMs, 
    visualize the heatmap and motif logo pairs in a simple grid
    '''
    model_weights = model_weights[0].squeeze(1)
    print(model_weights.shape)

    # make sure the model weights agree with the number of filters
    assert(model_weights.shape[0] == len(filter_activations))
    
    num_filts = len(filter_activations)
    num_rows = int(np.ceil(num_filts/num_cols))*2+1 
    # ^ not sure why +1 is needed... complained otherwise
    
    plt.figure(figsize=(20, 17))

    j=0 # use to make sure a filter and it's logo end up vertically paired
    for i, filter in enumerate(model_weights):
        if (i)%num_cols == 0:
            j += num_cols

        # display raw filter
        ax1 = plt.subplot(num_rows, num_cols, i+j+1)
        ax1.imshow(filter.cpu().detach(), cmap='gray')
        ax1.set_yticks(np.arange(4))
        ax1.set_yticklabels(['A', 'C', 'G','T'])
        ax1.set_xticks(np.arange(model_weights.shape[2]))
        ax1.set_title(f"Filter {i}")

        # display sequence logo
        ax2 = plt.subplot(num_rows, num_cols, i+j+1+num_cols)
        filt_df = pd.DataFrame(filter_activations[i].T.numpy(),columns=['A','C','G','T'])
        filt_df_info = logomaker.transform_matrix(filt_df,from_type='counts',to_type='information')
        logo = logomaker.Logo(filt_df_info,ax=ax2)
        ax2.set_ylim(0,2)
        ax2.set_title(f"Filter {i}")

    plt.tight_layout()
    

# using some seqs from test_df to activate filters
some_seqs = random.choices(seqs, k=3000)

filter_activations = get_filter_activations(some_seqs, conv_layers[0],act_thresh=1)
view_filters_and_logos(model_weights,filter_activations)


